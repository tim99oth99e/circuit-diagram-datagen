import torch
import torch.nn as nn
import torch.nn.functional as F


def circuit_decoder(input_dim, embed_dim, Ty):
    """Runs one forward pass of the decoder : 

    Args:
        input_dim (int): the dimension of the input = size of the dictionnary
        embed_dim (int): the dimension of the output vectors generated by the RNN
        Ty: (int): maximum number of time steps to generate

    returns:
        output: (List[int]): the list of tokens generated by the decoder
    """

    # define the nn layers (beware that they must share their weights for one sentence)
    lstm = nn.LSTM(input_dim, embed_dim)
    lin = nn.Linear(embed_dim, input_dim)
    sm = nn.Softmax(dim=-1)

    # define a random input first vector (will later be the one predicted by the encoder)
    input = [torch.randn(1, input_dim)]
    input = torch.cat(input).view(len(input), 1, -1)

    hidden = (torch.randn(1, 1, embed_dim), torch.randn(1, 1, embed_dim))

    tokens_list = []

    for t in range(Ty):
        # perfom one step on the LSTM cell
        out, hidden = lstm(input, hidden)
        # apply the fully connected layer to have an input-like vector
        out = lin(out)
        # apply the softmax to have a probability distribution over the input vector
        prob = sm(out)
        # get the index of the token with the highest score
        token = torch.argmax(prob, dim=-1)
        token_value = token.item()
        # if the token is the end of the sentence, exit the loop
        if token_value == 0:
            break
        tokens_list.append(token_value)

        # one-hot encode the token (for it to be the next input)
        input = F.one_hot(token, num_classes=input_dim)
        # convert to the right data type
        input = input.to(torch.float32)

    return tokens_list


if __name__ == "__main__":
    predicted_formula = circuit_decoder(input_dim=10, embed_dim=4, Ty=10)
    print(predicted_formula)
