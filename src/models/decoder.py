import torch
import torch.nn as nn
import torch.nn.functional as F

from scripts.preprocessing.preprocess_formulas import Vocabulary


class TextDecoder(nn.Module):
    def __init__(self, vocab: Vocabulary) -> None:
        """
        Args:
            vocab : (we can get vocab size, convert text tokens to OHE...)
            # formula_max_len (int) : maximal size of a formula
            # hidden_size : nb features (size) in the hidden state (= output)
        """
        super().__init__()
        self.vocab = vocab
        self.vocab_size = len(vocab)
        self.formula_max_len = vocab.formula_max_length
        self.lstm_cell = nn.LSTMCell(input_size=len(vocab), hidden_size=len(vocab))
        self.softmax = nn.Softmax(dim=-1)
        # later : add an embedding layer

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x (torch.Tensor): is a tensor of shape (batch_size, input_size)

        Returns:
            torch.Tensor: The tensor of one hot encoded tokens, generated by the decoder
        """
        # get SOS, EOS & PAD one-hot encoded vectors
        end_vect_id = self.vocab.word_to_idx["<EOS>"]
        pad_vect = self.vocab.get_encoded_token("<PAD>")
        sos_vect = self.vocab.get_encoded_token("<SOS>")

        # will contain the probabilities for each prediction of the different tokens
        # initialized with <PAD> token One Hot Encoded (and <SOS> for the first token)
        predictions = (
            torch.ones(size=(self.formula_max_len, self.vocab_size)) * pad_vect
        )
        predictions[0] = sos_vect

        # initialize hidden state with the encoder outputed vector <SOS> token
        hidden_state = x.squeeze()
        cell_state = torch.zeros(
            size=(self.vocab_size,)
        )  # how to initialize it properly ?
        # initialize the input vector with the <SOS> token
        input_vect = sos_vect.to(torch.float32)

        # until the full formula has been predicted,
        for i in range(self.formula_max_len - 1):
            # run once through the LSTM
            hidden_state, cell_state = self.lstm_cell(
                input_vect, (hidden_state, cell_state)
            )
            pred_probas = self.softmax(hidden_state)
            pred_token = pred_probas.argmax()
            predictions[i, :] = pred_probas

            # if the complete formula has been predicted, stop
            if pred_token == end_vect_id:
                break
        return predictions
